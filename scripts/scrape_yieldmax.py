#!/usr/bin/env python3
"""
Dynamic YieldMax Dividend Scraper
Automatically discovers and scrapes YieldMax dividend distribution articles from Globe Newswire.
Replaces all static URL-based YieldMax scrapers with dynamic article discovery.
"""

import sys
import time
import logging
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re
from scrape_dividend_calendar_requests import extract_table_data, save_to_dividend_calendar

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class YieldMaxDynamicScraper:
    def __init__(self, max_articles=20, days_back=365, include_product_launches=False):
        """
        Initialize the dynamic YieldMax scraper
        
        Args:
            max_articles: Maximum number of articles to process
            days_back: Only process articles from this many days back
            include_product_launches: Whether to process product launch articles (usually no dividend data)
        """
        self.max_articles = max_articles
        self.days_back = days_back
        self.include_product_launches = include_product_launches
        self.search_url = "https://www.globenewswire.com/en/search/keyword/yieldmax?pageSize=25"
        
    def discover_yieldmax_articles(self):
        """Discover YieldMax articles from Globe Newswire search"""
        logger.info(f"ğŸ” Discovering YieldMax articles from Globe Newswire...")
        print(f"ğŸ” Searching for YieldMax articles...")
        
        try:
            response = requests.get(self.search_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Find article links
            article_links = []
            
            # Look for links in search results
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                text = link.get_text().strip()
                
                if 'news-release' in href and 'YieldMax' in text:
                    if href.startswith('/'):
                        href = 'https://www.globenewswire.com' + href
                    
                    # Extract date from URL for filtering
                    date_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', href)
                    if date_match:
                        year, month, day = map(int, date_match.groups())
                        article_date = datetime(year, month, day)
                        cutoff_date = datetime.now() - timedelta(days=self.days_back)
                        
                        if article_date >= cutoff_date:
                            article_links.append({
                                'url': href,
                                'title': text,
                                'date': article_date
                            })
            
            # Remove duplicates and sort by date (newest first)
            seen_urls = set()
            unique_articles = []
            for article in article_links:
                if article['url'] not in seen_urls:
                    seen_urls.add(article['url'])
                    unique_articles.append(article)
            
            unique_articles.sort(key=lambda x: x['date'], reverse=True)
            
            logger.info(f"âœ… Discovered {len(unique_articles)} YieldMax articles within {self.days_back} days")
            return unique_articles[:self.max_articles]
            
        except Exception as e:
            logger.error(f"âŒ Error discovering articles: {e}")
            return []
    
    def filter_dividend_articles(self, articles):
        """Filter articles to focus on dividend distributions vs product launches"""
        dividend_articles = []
        other_articles = []
        
        for article in articles:
            title = article['title'].lower()
            url = article['url'].lower()
            
            # Keywords that indicate dividend distribution articles
            dividend_keywords = [
                'announces distributions',
                'distribution',
                'dividend',
                'monthly distributions',
                'quarterly distributions'
            ]
            
            # Keywords that indicate product launches (usually no dividend data)
            launch_keywords = [
                'introduces',
                'launches',
                'new etf',
                'option income strategy etf'
            ]
            
            is_dividend_article = any(keyword in title for keyword in dividend_keywords)
            is_launch_article = any(keyword in title for keyword in launch_keywords)
            
            if is_dividend_article or (is_launch_article and self.include_product_launches):
                if is_dividend_article:
                    dividend_articles.append(article)
                else:
                    other_articles.append(article)
        
        logger.info(f"ğŸ“Š Filtered articles: {len(dividend_articles)} dividend, {len(other_articles)} other")
        
        return dividend_articles, other_articles
    
    def scrape_articles(self, articles, article_type="dividend"):
        """Scrape dividend data from discovered articles"""
        total_saved = 0
        successful_articles = 0
        
        logger.info(f"ğŸš€ Starting dynamic YieldMax scraping - {len(articles)} {article_type} articles")
        print(f"ğŸ¯ Dynamic YieldMax Scraper")
        print("=" * 80)
        print(f"ğŸ“° Total {article_type} articles to process: {len(articles)}")
        print(f"ğŸ—“ï¸ Date range: Last {self.days_back} days")
        print(f"ğŸ” Auto-discovered from: {self.search_url}")
        print("=" * 80)
        
        for i, article in enumerate(articles, 1):
            try:
                logger.info(f"ğŸ“° Processing article {i}/{len(articles)}: {article['url']}")
                print(f"\nğŸ“° Article {i}/{len(articles)} - {article['date'].strftime('%B %d, %Y')}")
                print(f"ğŸ“„ Title: {article['title']}")
                print(f"ğŸ”— URL: {article['url']}")
                
                # Extract data from this article
                data = extract_table_data(article['url'])
                
                if data:
                    logger.info(f"âœ… Found {len(data)} dividend records in article {i}")
                    print(f"âœ… Found {len(data)} dividend records")
                    
                    # Display found records
                    for record in data:
                        print(f"  â€¢ {record['ticker']}: ${record['amount']} (ex: {record['ex_date']}, pay: {record['payment_date']})")
                    
                    # Save to database
                    saved_count = save_to_dividend_calendar(data)
                    total_saved += saved_count
                    successful_articles += 1
                    
                    logger.info(f"ğŸ’¾ Saved {saved_count} records from article {i}")
                    print(f"ğŸ’¾ Saved {saved_count} records to database")
                    
                else:
                    logger.warning(f"âš ï¸ No dividend data found in article {i}")
                    print(f"âš ï¸ No dividend data found")
                    if "introduces" in article['title'].lower() or "launches" in article['title'].lower():
                        print("  (This appears to be a product launch announcement)")
                
                # Brief pause between articles
                if i < len(articles):
                    logger.info("â³ Pausing 3 seconds before next article...")
                    time.sleep(3)
                    
            except KeyboardInterrupt:
                logger.info("â¹ï¸ User cancelled operation")
                break
            except Exception as e:
                logger.error(f"âŒ Error processing article {i}: {e}")
                print(f"âŒ Error: {e}")
                continue
        
        return total_saved, successful_articles
    
    def run(self):
        """Main scraping workflow"""
        try:
            # Discover articles
            all_articles = self.discover_yieldmax_articles()
            
            if not all_articles:
                print("âš ï¸ No YieldMax articles found")
                return 0
            
            # Filter articles
            dividend_articles, other_articles = self.filter_dividend_articles(all_articles)
            
            # Process dividend articles first (priority)
            total_saved = 0
            total_successful = 0
            
            if dividend_articles:
                print(f"\nğŸ¯ Processing {len(dividend_articles)} dividend distribution articles...")
                saved, successful = self.scrape_articles(dividend_articles, "dividend")
                total_saved += saved
                total_successful += successful
            
            # Process other articles if enabled
            if other_articles and self.include_product_launches:
                print(f"\nğŸš€ Processing {len(other_articles)} product launch articles...")
                saved, successful = self.scrape_articles(other_articles, "product launch")
                total_saved += saved
                total_successful += successful
            
            # Summary
            print("\n" + "=" * 80)
            print(f"ğŸ‰ Dynamic YieldMax Scraping Complete!")
            print(f"ğŸ“Š Results Summary:")
            print(f"  â€¢ Articles discovered: {len(all_articles)}")
            print(f"  â€¢ Dividend articles: {len(dividend_articles)}")
            print(f"  â€¢ Other articles: {len(other_articles)}")
            print(f"  â€¢ Articles processed: {total_successful}")
            print(f"  â€¢ Total dividend records saved: {total_saved}")
            print(f"  â€¢ Date range: Last {self.days_back} days")
            print(f"  â€¢ Auto-discovery: âœ… Enabled")
            print("=" * 80)
            
            logger.info(f"ğŸ Dynamic scraping complete: {total_successful} articles processed, {total_saved} total records saved")
            
            # Show database totals
            if total_saved > 0:
                try:
                    import os
                    from dotenv import load_dotenv
                    from supabase import create_client
                    
                    load_dotenv()
                    url = os.getenv('SUPABASE_URL')
                    key = os.getenv('SUPABASE_KEY')
                    supabase = create_client(url, key)
                    
                    total_result = supabase.table('dividend_calendar').select('*', count='exact').execute()
                    print(f"ğŸ“ˆ Updated database total: {total_result.count} dividend records")
                    
                except Exception as e:
                    logger.warning(f"Could not get database total: {e}")
            
            return total_saved
            
        except KeyboardInterrupt:
            logger.info("â¹ï¸ Operation cancelled by user")
            print("\nâ¹ï¸ Operation cancelled")
            return 0
        except Exception as e:
            logger.error(f"âŒ Fatal error in dynamic scraper: {e}")
            print(f"\nâŒ Fatal error: {e}")
            return 0

def main():
    """Main entry point with configurable options"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Dynamic YieldMax Dividend Scraper')
    parser.add_argument('--max-articles', type=int, default=20, help='Maximum articles to process (default: 20)')
    parser.add_argument('--days-back', type=int, default=90, help='Days back to search (default: 90)')
    parser.add_argument('--include-launches', action='store_true', help='Include product launch articles')
    parser.add_argument('--recent-only', action='store_true', help='Only process last 7 days')
    
    args = parser.parse_args()
    
    # Adjust days back for recent-only mode
    days_back = 7 if args.recent_only else args.days_back
    
    logger.info("ğŸ¯ YieldMax Dynamic Scraper Starting")
    print(f"ğŸ¯ YieldMax Dynamic Dividend Scraper")
    print(f"âš™ï¸ Max Articles: {args.max_articles}")
    print(f"ğŸ“… Days Back: {days_back}")
    print(f"ğŸš€ Include Launches: {args.include_launches}")
    print("")
    
    scraper = YieldMaxDynamicScraper(
        max_articles=args.max_articles,
        days_back=days_back,
        include_product_launches=args.include_launches
    )
    
    total_saved = scraper.run()
    
    if total_saved > 0:
        print(f"\nâœ… Successfully saved {total_saved} YieldMax dividend records")
    else:
        print(f"\nğŸ“ No new dividend records found or saved")

if __name__ == "__main__":
    main()